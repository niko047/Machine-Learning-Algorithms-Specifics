{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Theory behind the Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Logistic Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is a **classification** algorithm (our response y is categorical). The main goal of this classifier is to map each input to a vector of probabilities indicating the probability of that observation belonging to the response categories. Say our **input** is multiple vectors of the kind\n",
    "\n",
    "$$x = \\begin{pmatrix} x_1, & ..., & x_m \\end{pmatrix}$$\n",
    "\n",
    "And suppose our **output** lives in the following set $y \\in \\{0, 1\\}$\n",
    "\n",
    "We can now define what we are eventually interested in, meaning the probability of our input to belong in a certain category, which in this case takes the form of $$p(x) = P(y=1|x) = 1 - P(y=0|x)$$\n",
    "\n",
    "It is possible to mathematically derive the logistic function (I am displaying just the binary case for simplicity, but similar results hold for the multivariate case), obtaining\n",
    "\n",
    "$$p(x) = \\frac {e^{\\beta_0 + \\beta_1  x_1 + ... + \\beta_m  x_m}}{1 + e^{\\beta_0 + \\beta_1  x_1 + ... + \\beta_m  x_m}}$$\n",
    "\n",
    "Therefore we can train training our algorithm on the data, so that to build the parameters $\\beta_0, ..., \\beta_m$ and to estimate $\\hat{y_i}$ for $\\forall{i} \\in \\{0, ..., n\\}$, which is usually $\\hat{y_i} = \\unicode{x1D7D9}(\\hat{p}(x_i)\\geq0.5)$, where $\\unicode{x1D7D9}$ is the indicator function returning True or False (1 or 0), whether the function condition is satisfied or not. Meaning that if the predicted probability for input $i$ is greater than $\\frac{1}{2}$, it gets predicted to belong to class 1, otherwise 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When should I use it? What data should I feed it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>It works well with <strong>continuous data types</strong></li>\n",
    "<li>Categorical variables need to be <strong>OneHotEncoded</strong></li>\n",
    "<li>Logistic Regression assumes the data is linearly or curvy linearly separable in space (whereas trees do not)</li>\n",
    "<li>This algorithm <strong>does not handle skewed classes well</strong>. Say if 75% of the output is 1 and just 25% is 0, then we need to <strong>balance</strong> the classes or play with the <strong>class weights</strong></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moral of the story: it is a good and reliable algorithm, yet its range of application can be considered narrow when compared to other algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Practical Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose I am interested in whether a person has survived the titanic disaster or not. As an emergency protocol the ship evacuated first the young, the elder and the women. Let us analyze the latter category with a logistic regression, checking whether the characteristic of being a female has had an impact on the probability of survival of a passenger. For those purposes I will use the titanic dataset, obtained through the seaborn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I am importing the necessary dependencies\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "pd.options.mode.chained_assignment = None\n",
    "df = sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
       "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
       "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
       "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
       "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
       "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
       "\n",
       "     who  adult_male deck  embark_town alive  alone  \n",
       "0    man        True  NaN  Southampton    no  False  \n",
       "1  woman       False    C    Cherbourg   yes  False  \n",
       "2  woman       False  NaN  Southampton   yes   True  \n",
       "3  woman       False    C  Southampton   yes  False  \n",
       "4    man        True  NaN  Southampton    no   True  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now check how many unique types of gender characterize the individuals on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['male', 'female'], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sex'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Male and Female are the only genders present in the dataset, thus we have a binary output.\n",
    "I will now select the input and output columns I'm interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sex_survived = df[['sex','survived']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now the values under the column \"sex\" are strings, 'male' and 'female', I want to transform the column into a dummy variable, having value 1 if the individual is a female, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sex_survived['sex'] = df_sex_survived['sex'].apply(lambda x: 1 if x=='female' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sex  survived\n",
       "0      0         0\n",
       "1      1         1\n",
       "2      1         1\n",
       "3      1         1\n",
       "4      0         0\n",
       "..   ...       ...\n",
       "886    0         0\n",
       "887    1         1\n",
       "888    1         0\n",
       "889    0         1\n",
       "890    0         0\n",
       "\n",
       "[891 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sex_survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now store part of my dataset for training purposes, and I will leave out another portion of it for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X= df_sex_survived['sex'].values.reshape(-1, 1) #reshaping it because it has a single feature\n",
    "y = df_sex_survived['survived'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.7849117174959872 %\n"
     ]
    }
   ],
   "source": [
    "logistic_reg = LogisticRegression(random_state=0)\n",
    "logistic_reg.fit(X_train,y_train)\n",
    "print('Accuracy score:',logistic_reg.score(X_train, y_train),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have an accuracy score of around 80%, which means simply by knowing whether a passenger was a male of a female, we would be correct 80% of the times in classifying whether he/she has survived the titanic accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of the regression 2.370100620067801\n"
     ]
    }
   ],
   "source": [
    "print('Coefficient of the regression', logistic_reg.coef_[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the interpretability of coefficient in the logistic regression is not straightforward, but we can make some observations by looking at the positive sign, telling us that the input feature does on average have a positive impact, contributing to a probability increase in survival rate when true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Parameters in Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** in Sklearn, when we use LinearRegression, regularization of the input is applied by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>LogisticRegression</em> (<em>penalty</em>='l2', *, <em>dual</em>=False, <em>tol</em>=0.0001, <em>C</em>=1.0, <em>fit_intercept</em>=True, <em>intercept_scaling</em>=1, <em>class_weight</em>=None, <em>random_state</em>=None, <em>solver</em>='lbfgs', <em>max_iter</em>=100, <em>multi_class</em>='auto', <em>verbose</em>=0, <em>warm_start</em>=False, <em>n_jobs</em>=None, <em>l1_ratio</em>=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penalty refers to how mispredictions are penalized**. The possible options offered by sklearn are **\"$\\ell_2$\", \"$\\ell_1$\"** and **\"elasticnet\"**. The norm we should use depends on our endgoal. Using the l2 norm, we keep all of our coefficient, with the least significant ones dropping significantly in size. Using the l1 norm, we force the coefficients that are less significant to go to zero, which we then do not consider. On the other hand, elasticnet is a combination of the two, at the expense of more computational power needed, it is more flexible, which can be a pro, though it should be weighted with the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding a little more on the topic, the mathematical structure of an $\\ell_p$ norm is defined as $\\ell_p = (\\sum_{i}^{m} |{x_i^p}|)^{1/p}$, where p denoted the type of norm. In our case, the norm regularization is applied on our coefficient $\\beta$ 's, thus we have that $$\\ell_1 = ||\\beta||_1 =\\sum_{i}^{m} |\\beta_i|$$ $$\\ell_2 = ||\\beta||_2 = (\\sum_{i}^{m} |\\beta_i^2|)^{1/2}$$ $$\\ell_{elasticnet}=\\alpha||\\beta||_2 + (1-\\alpha)||\\beta||_1$$ where $$\\alpha = \\frac{\\lambda_2}{(\\lambda_1 + \\lambda_2)}$$\n",
    "<em>More infos on elasticnet can be found at https://web.stanford.edu/~hastie/TALKS/enet_talk.pdf</em>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fit_intercept** is a boolean parameter deciding whether the intercept $\\beta_0$ should be inserted in the model or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**class_weight** referes to the balancing of the classes, when the class training data is skewed. For example if we're predicting a binary label 0 or 1, and the vast majority of our training dataset contains values classified as 1, having way less values classified at 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**random_state** is the random seed that shuffles the data. By fixing it, we are able to compare different models within constant subsets of data, eliminating differences originating by differences in subsets of data selected for the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**solver** refers to the algorithm used in the maximimation process. The decision should be made according to the size of the dataset and the amount of classes we want to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**max_iter** refers to the number of maximum iterations of the algorithm we are able to tollerate. If the algorithm does not converge, theoretically should run forever. By setting a large yet reasonable amount of iterations we should be able to achieve close to convergence even if it will wiggle forever around the minimum of the optimization problem, yet never settling there specifically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**multi_class** refers to the loss to be used in the particular case. If we are dealing with a multilabel classification we should change it to be \"multinomial\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
